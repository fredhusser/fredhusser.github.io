<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Fundamentals on topic modeling While the tf-idf technique, which we went through in the past post, is very efficient for extracting features that are discriminative to the documents, it suffers from ...">
        <meta name="keywords" content="Latent Dirichlet Allocation, Latent Semantic Indexing, Machine Learning, Scikit-Learn, Vector Space Model">
        <link rel="icon" href="./favicon.ico">

        <title>Part 3C - Topic modeling by semantic analysis in Python with Scikit-Learn - Data Science for collective intelligence</title>

        <!-- Stylesheets -->
        <link href="./theme/css/bootstrap.min.css" rel="stylesheet">
        <link href="./theme/css/fonts.css" rel="stylesheet">
        <link href="./theme/css/nest.css" rel="stylesheet">
        <link href="./theme/css/pygment.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <link href="https://fredhusser.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Data Science for collective intelligence Full Atom Feed" />
        <link href="https://fredhusser.github.io/feeds/python.atom.xml" type="application/atom+xml" rel="alternate" title="Data Science for collective intelligence Categories Atom Feed" />
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container gradient">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./">Data Science for collective intelligence</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./">Homepage</a>
                                <a href="./categories.html">Categories</a>
                            <a  href="./pages/about.html">about</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">Part 3C - Topic modeling by semantic analysis in Python with Scikit-Learn</h1>
                      <p class="header-date">By <a href="./author/frederic-husser.html">Frederic Husser</a>, mer. 16 d√©cembre 2015, in category <a href="./category/python.html">Python</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/latent-dirichlet-allocation.html">Latent Dirichlet Allocation</a>, <a href="./tag/latent-semantic-indexing.html">Latent Semantic Indexing</a>, <a href="./tag/machine-learning.html">Machine Learning</a>, <a href="./tag/scikit-learn.html">Scikit-Learn</a>, <a href="./tag/vector-space-model.html">Vector Space Model</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <h2>Fundamentals on topic modeling</h2>
<p>While the tf-idf technique, which we went through in <a href="./feature_extraction.html">the past post</a>, is very efficient for extracting features that are discriminative to the documents, it suffers from several drawbacks and limitations.</p>
<ul>
<li>Straight feature extraction results in high dimensional and sparse feature spaces. With very conservative filtering parameters we came out with almost 5000 features describing our corpus of 2000 documents. Analyzing matrices of such size always comes always at high computational costs. Furthermore, in the field of machine learning, algorithms usually involve iterative training algorithms, that enhance the computational cost and might also be prone to overfitting. Dimensionality reduction is first and foremost a matter of efficiency. How could the input space be as small as possible while being the most descriptive and discriminative possible?</li>
<li>Sparsity in itself is also a problem when computing cosine similarity (dot product) between documents in their vector space representation. Indeed, since the number of features in a single document is far off the total amount of features, the non-zero components are very rare. In the end, the probability to find common non-zero components in two document vectors is low. </li>
<li>Sole feature extraction completely flattens the documents in terms of series of words or n-grams. These feature have no inner relationships. In natural languages, the reality is the opposite. Indeed, words do have relationships with each others, in terms of syntax but also in terms of topics. In addition to that polysemy and homonymy is very important. Polysemy is the fact that single words can be related to different meanings and topics. When not taken into account it leads to overestimating the similarity between documents. Similarly homonymy is the fact that several words can refer to the same meaning, and therefore lead to an underestimation of the true similarity between documents.</li>
</ul>
<p>For overcoming these limits, researchers have come with solution that must be operated once features have been extracted and before performing comparisons and classification tasks on the documents. This is the object of topic modeling.</p>
<h3>Latent semantic indexing (LSI)</h3>
<p>The first dimensionality reduction technique, and the easiest to understand and implement as well, is the <strong>latent semantic indexing (LSI)</strong>. It consists in using a singular value decomposition of the documents-terms matrix. Thus a linear subspace in the TF-IDF high dimensional space is found, which better captures the internal relationships between features. The singular value decomposition matrix, truncated to reduce to output space, known as the laten tspace, to around 300 dimensions. This allows to define new features which are linear combinations between the features of the input TF-IDF space. </p>
<p>The components of the features in the can be positive and negative. Within a given feature, the highest positive components represent the TF-IDF features which at best describe the topic, while the highest negative components represent the TF-IDF features that are the most repulsive to that topic.</p>
<p>LSI is very fast, and requires a low computational time. The algorithm is based on the Singular Value Decomposition of the TF-IDF matrix followed by a normalization. The SVD matrix is truncated to the size of the required feature space. Two matrices are then available: the documents-topics matrix and the topics-features matrix. </p>
<ul>
<li>The first one is the linear mapping from documents to topics, with M documents and K topics. This matrix is dense and maps each documents linearly to the set of topics. This matrix with positive components is becomining our input data for the classification task, as it thought to be more consistant to the actual content of the text.</li>
<li>The second matrix is the linear mapping from the topics to the features, with N features and K topics. This matrix is used for retrieving the description of a topic, in terms of highest components.</li>
</ul>
<h3>Probabilistic Latent Semantic Indexing (pLSI)</h3>
<p>Simple LSI have come with shortcomings. One immediate concern is the fact that features are linearly combined with positive and negative components. Negative components are difficult to interpret in terms of meanings for a topic.</p>
<p>A second issue is that the LSI technique is non-generative for documents-words co-occurences; changing the nature of one of the documents will change the full LSI matrix. From the perspective of building a web service, this is a really annoying issue, in that the analysis cannot be reused by previously unseen documents, and even if the document are slightly modified.</p>
<p>As it has been stated by Hofmann in <a href="http://www.cs.helsinki.fi/u/vmakinen/stringology-k04/hofmann-unsupervised_learning_by_probabilistic_latent_semantic_analysis.pdf">is publication on pLSI</a> LSI is unable to capture polysemy of the features. "The coordinates of the of a word in the latent space can be written as a linear superposition of the coordinates of the documents that contain the word". The fact that a word could have different meaning in different documents cannot be explained by this superposition model.</p>
<p>Hofmann et al. have thus built a technique close to the latent semantic analysis model tackling these issues more seriously. The pLSI model adds a latent variable related to each single occurrence for a feature. It is used to give information about the context of the occurrence, that is to say the other words also appearing in the document. This helps accounting for polysemy. For instance, consider the word python. A first common sense is the snake, and a second possible meaning is the programming language. Given the other words co-occurring in the same document, it can be inferred whether the sense of the occurrence is the snake - for example when the word 'snake' is also in the document, or the right meaning is the programming language. In the last case, words like 'program', 'computer' or 'java' are likely to be present.</p>
<p>A common representation of this algorithm is given in the following figure, found in the <a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">Wikipedia article about pLSI</a>. This is a plate notation where the relationships between variables are displayed.
<img src="./images/plsi.png" title="'Plate representation of the PLSI model'" height="150" width="300" alt="'Plate representation of the PLSI model'" class="left half"></p>
<p>d is the document index variable. For each of the M documents, a word's topic c is extracted with a distribution over the N words w present in the full corpus. Using this notation, the co-occurrences are calculated in terms of probabilities P(w,d). These probabilities are decomposed in terms of conditional probabilities involving the latent variable c. The variable c is said to be latent in that it is not observable. In the application of topic modeling, the latent classes can be seen as topics as they create distributions of words likely to be present together in documents. </p>
<h3>Latent Dirichlet Allocation</h3>
<p>The implementation of topic modeling with the probabilistic Latent Semantic Indexing, was proved to have a serious limitation. It was <a href="http://jmlr.csail.mit.edu/papers/v3/blei03a.html">shown by Blei, Ng and Jordan</a> that this algorithm is not generative for documents. This means that the topic models which can be extracted for an analysis work poorly with un-seen documents. This is a serious issue in our perspective of classifying great amounts of documents: we need the analysis to be performed on a retricted set of documents while being able to apply the model to unseen documents whenever some are added to the corpus. </p>
<p>The plate notation of LDA is shown below from the <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Wikipedia article about LDA</a>.
<img src="./images/lda.png" title="'Plate representation of the LDA model'" height="150" width="300" alt="'Plate representation of the LDA model'" class="left half"></p>

<div class="twitter">
    <a href="https://twitter.com/share" class="twitter-share-button"{count} data-size="large" data-related="fredhusser">Tweet</a>
    <script>!
        function(d,s,id){
            var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';
            if(!d.getElementById(id)){
                js=d.createElement(s);
                js.id=id;
                js.src=p+'://platform.twitter.com/widgets.js';
                fjs.parentNode.insertBefore(js,fjs);
            }
        }(document, 'script', 'twitter-wjs');
    </script>
</div><div class="comments">
    <div id="disqus_thread"></div>
    <script>
        var disqus_config = function () {
            this.page.url = 'https://fredhusser.github.io/semantic-indexing.html'; // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = 'semantic-indexing.html'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');

            s.src = '//fredhusser.disqus.com/embed.js';

            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments
        powered by Disqus.</a></noscript>
</div>
        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                            <li><a href="./authors.html">Authors</a></li>
                            <li><a href="https://fredhusser.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"></a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Social</div>
                        <ul class="list-unstyled">
                            <li><a href="https://github.com/fredhusser" target="_blank">GitHub</a></li>
                            <li><a href="http://twitter.com/fredhusser" target="_blank">twitter</a></li>
                            <li><a href="https://fr.linkedin.com/in/fr√©d√©ric-husser-29bb0717" target="_blank">linkedIn</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Links</div>
                        <ul class="list-unstyled">
                            <li><a href="#" target="_blank">Home</a></li>
                        </ul>
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; blogname 2015</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>