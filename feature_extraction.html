<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="In the introduction post we have described the workflow we want to create for performing an automatic classification of text documents. These text documents have been crawled on lemonde.fr website. ...">
        <meta name="keywords" content="GENSIM, Machine Learning, NLTK, Scikit-Learn, TF-IDF, Vector Space Model">
        <link rel="icon" href="./favicon.ico">

        <title>Part 3B - Text documents feature extraction with Scikit-Learn - Data Science for collective intelligence</title>

        <!-- Stylesheets -->
        <link href="./theme/css/bootstrap.min.css" rel="stylesheet">
        <link href="./theme/css/fonts.css" rel="stylesheet">
        <link href="./theme/css/nest.css" rel="stylesheet">
        <link href="./theme/css/pygment.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <link href="https://fredhusser.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Data Science for collective intelligence Full Atom Feed" />
        <link href="https://fredhusser.github.io/feeds/python.atom.xml" type="application/atom+xml" rel="alternate" title="Data Science for collective intelligence Categories Atom Feed" />
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container gradient">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./">Data Science for collective intelligence</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./">Homepage</a>
                                <a href="./categories.html">Categories</a>
                            <a  href="./pages/about.html">about</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">Part 3B - Text documents feature extraction with Scikit-Learn</h1>
                      <p class="header-date">By <a href="./author/frederic-husser.html">Frederic Husser</a>, mar. 15 décembre 2015, in category <a href="./category/python.html">Python</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/gensim.html">GENSIM</a>, <a href="./tag/machine-learning.html">Machine Learning</a>, <a href="./tag/nltk.html">NLTK</a>, <a href="./tag/scikit-learn.html">Scikit-Learn</a>, <a href="./tag/tf-idf.html">TF-IDF</a>, <a href="./tag/vector-space-model.html">Vector Space Model</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <p>In the <a href="./build_classifier.html">introduction post</a> we have described the workflow we want to create for performing an automatic classification of text documents. These text documents have been crawled on lemonde.fr website.</p>
<h2>Text vectorization</h2>
<p>For vectorizing the corpus of articles we create a <code>Model</code> child class called <code>ModelTFIDF</code>. It uses the TF-IDF vectorization algorithm from Scikit-Learn. If you are not familiar to the Scikit-Learn interfaces,<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">the documentation</a> can be consulted relatively to the <code>TfidfVectorizer</code>. </p>
<p>The underlying algorithm is based on a count vectorizer and a tf-idf transformer. </p>
<ul>
<li>The first tokenizes the text documents and makes a reference of all tokens with their appearance frequency in each document. A count matrix is built with tokens as columns and documents as rows. Since all words are not appearing in all documents the count matrix is very sparse. Especially when working with other languages than English, a very high level implementation of the tokenizer is available in the NLTK (Natural Language Toolkit). In a future step, a French tokenizer will be used. It applies a stemming algorithm, so that words like 'manger' and 'mangé' are not counted as different. Stemming can be highly beneficial in terms of computing time and also improves the relevance of the results.</li>
<li>The tf-idf transformer operates a modulation of the in-document term frequencies by the log of the inverse of the document frequency for each feature. In other words, each time that a words appears at least once in a document of the corpus this is </li>
</ul>
<p>We first create the class, and override the metadata, namely the name of the output data. This is mainly required for producing consistant logs during the fit process.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">ModelTFIDF</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s">&quot;transformer&quot;</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">&quot;tfidf&quot;</span>
    <span class="n">output_data_name</span> <span class="o">=</span> <span class="s">&quot;tfidfMatrix&quot;</span>
    <span class="n">mapper_name</span> <span class="o">=</span> <span class="s">&quot;words_to_index&quot;</span>
</pre></div>


<p>Obviously we need to override the <code>fit</code> method. In this method we instantiate the vectorizer from Scikit-Learn with all the required parameters. Note that the object attribute <code>fit_parameters</code> is set in the standardized <code>__init__</code> method. Using the vectorizer from Scikit-Learn, we have to load the input data as a numpy array, which is the type requirement of the object attribute <code>input_data</code> as instantiated in the <code>__init__</code> method as well.</p>
<p>The vectorizer outputs a sparse documents-features matrix. This will be assigned to our <code>output_data</code> attributes. The <code>mapper_data</code> attributes takes the vectorizer dictionary which maps the indexes of the features in the documents-features matrix and the string representation of the feature. In the most basic set-up a feature is a word. But it could also be bi-grams (two consecutive words), or more.</p>
<div class="highlight"><pre>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_tokenizer</span><span class="p">(</span><span class="n">language</span><span class="p">)</span>
        <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_parameters</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_data</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_space_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c"># Give access to the index of a word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mapper_data</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span>

        <span class="c"># Return a reverse mapper for retrieving a word from index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mapper_reverse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_space_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">&quot;a30&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapper_data</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapper_reverse</span><span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="n">key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_model_results</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>
</pre></div>


<p>The <code>TfIdfVectorizer</code> fit parameters are stored as Python dictionary in the <code>MLconfig.py</code> module. The key-value pairs are then passed to the vectorizer instance in the fit method. The most important parameters at first are the <code>max_df</code> and <code>min_df</code>: they set the upper and lower thresholds for filtering out features considering their document frequencies. The document frequency is calculated relatively to the occurance in the corpus:</p>
<ul>
<li>Words or features appearing in all documents tend to be non discriminating in terms of documents comparisons. Usually these are what are called the stop-words: these are common to the language and grammar, and do not add much information on the content. In our example, we filter out features with more than 60% document frequency.</li>
<li>Words or features appearing in a very small amount of documents tend to so specific that they cannot be used for a proper comparison. In the similarity measure that we will use in the classifier based on a dot product, they will constantly account for a multiplication by zero. Therefore they will be responsible a high computational while bringing low insight. In our example, we filter out features with less than 1% document frequency.</li>
</ul>
<div class="highlight"><pre><span class="n">PARAMETERS_VECTORIZE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&quot;vocabulary&quot;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
    <span class="s">&quot;max_df&quot;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>
    <span class="s">&quot;min_df&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="s">&quot;ngram_range&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="c"># unigrams or bigrams</span>
    <span class="s">&quot;encoding&quot;</span><span class="p">:</span> <span class="s">&quot;utf-8&quot;</span><span class="p">,</span>
    <span class="s">&quot;strip_accents&quot;</span><span class="p">:</span> <span class="s">&#39;ascii&#39;</span><span class="p">,</span>
    <span class="s">&quot;norm&quot;</span><span class="p">:</span> <span class="s">&#39;l2&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>


<p>Using the workflow class defined in <a href="./build_classifier.html">Part A</a>, we add a method for vectorizing the documents inputs. This method return the <code>self</code> keyword for easing the chaining of processors in single command lines.</p>
<div class="highlight"><pre>    <span class="k">def</span> <span class="nf">vectorizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Representation of the text data into the vector space model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectorize_</span> <span class="o">=</span> <span class="n">ModelTFIDF</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_data</span><span class="o">.</span><span class="n">body</span><span class="p">,</span> <span class="n">PARAMETERS_VECTORIZE</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attributes</span><span class="p">[</span><span class="s">&quot;corpus_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectorize_</span><span class="o">.</span><span class="n">training_set_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attributes</span><span class="p">[</span><span class="s">&quot;n_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectorize_</span><span class="o">.</span><span class="n">output_space_size</span>
        <span class="k">return</span> <span class="bp">self</span>
</pre></div>


<p>Then a workflow object must be instantiated with the MongoDB connection parameters, and the <code>vectorizer</code> method is operated. In a Python shell at the root of the subproject, you can fire-up the vectorization in one single command line. The log displays information about the processed job.</p>
<div class="highlight"><pre>&gt;&gt;&gt; from textmining.workflows import SemanticAnalysisWorkflow
&gt;&gt;&gt; SemanticAnalysisWorkflow<span class="o">(</span>mongoclient, <span class="s2">&quot;scrapy&quot;</span>, <span class="s2">&quot;lemonde&quot;</span><span class="o">)</span>.vectorizer<span class="o">()</span>
tfidf:  Model tfidf of <span class="nb">type </span>transformer was trained
tfidf:  Output name:     tfidfMatrix
tfidf:  Input space:     <span class="o">(</span>1958,<span class="o">)</span>
tfidf:  Output space:    <span class="o">(</span>1958, 4777<span class="o">)</span>
tfidf:  Model attributes:
</pre></div>


<p>In our example, we have a corpus of 1958 text documents. After vectorization, a document matrix of shape 1958 documents by 4777 features. Obviously, the number of features is rather limited for such a high number of documents. However, for testing purposes, with the only goal to set up a full workflow this is more than enough. At the end of the iteration, we will be able to review the full process and make right decisions on rather or not we should filter so many features.</p>
<p>Since the vectorizer returns the object itself, we could store it in a variable in order to reuse it later, with a second model. In the <a href="https://github.com/fredhusser/collective-intelligence">Github repo</a>, a IPython notebook can be used for better understanding the mechanics of the vectorization model.</p>

<div class="twitter">
    <a href="https://twitter.com/share" class="twitter-share-button"{count} data-size="large" data-related="fredhusser">Tweet</a>
    <script>!
        function(d,s,id){
            var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';
            if(!d.getElementById(id)){
                js=d.createElement(s);
                js.id=id;
                js.src=p+'://platform.twitter.com/widgets.js';
                fjs.parentNode.insertBefore(js,fjs);
            }
        }(document, 'script', 'twitter-wjs');
    </script>
</div><div class="comments">
    <div id="disqus_thread"></div>
    <script>
        var disqus_config = function () {
            this.page.url = 'https://fredhusser.github.io/feature_extraction.html'; // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = 'feature_extraction.html'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');

            s.src = '//fredhusser.disqus.com/embed.js';

            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments
        powered by Disqus.</a></noscript>
</div>
        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                            <li><a href="./authors.html">Authors</a></li>
                            <li><a href="https://fredhusser.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"></a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Social</div>
                        <ul class="list-unstyled">
                            <li><a href="https://github.com/fredhusser" target="_blank">GitHub</a></li>
                            <li><a href="http://twitter.com/fredhusser" target="_blank">twitter</a></li>
                            <li><a href="https://fr.linkedin.com/in/frédéric-husser-29bb0717" target="_blank">linkedIn</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Links</div>
                        <ul class="list-unstyled">
                            <li><a href="#" target="_blank">Home</a></li>
                        </ul>
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; blogname 2015</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>