<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Science for collective intelligence</title><link href="https://fredhusser.github.io/" rel="alternate"></link><link href="https://fredhusser.github.io/feeds/python-tags-machine-learning-scikit-learn-vector-space-model-latent-semantic-indexing-latent-dirichlet-allocation.atom.xml" rel="self"></link><id>https://fredhusser.github.io/</id><updated>2015-12-16T12:00:00+01:00</updated><entry><title>Part 3C - Topic modeling by semantic analysis in Python with Scikit-Learn</title><link href="https://fredhusser.github.io/semantic-indexing%20Authors:%20Frederic%20Husser.html" rel="alternate"></link><updated>2015-12-16T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-16:semantic-indexing Authors: Frederic Husser.html</id><summary type="html">&lt;h2&gt;Fundamentals on topic modeling&lt;/h2&gt;
&lt;p&gt;While the tf-idf technique, which we went through in &lt;a href="%7bfilename%7dProg_Part3B_Feature-Extraction.md"&gt;the past post&lt;/a&gt;, is very efficient for extracting features that are discriminative to the documents, it suffers from several drawbacks and limitations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Straight feature extraction results in high dimensional and sparse feature spaces. With very conservative filtering parameters we came out with almost 5000 features describing our corpus of 2000 documents. Analyzing matrices of such size always comes always at high computational costs. Furthermore, in the field of machine learning, algorithms usually involve iterative training algorithms, that enhance the computational cost and might also be prone to overfitting. Dimensionality reduction is first and foremost a matter of efficiency. How could the input space be as small as possible while being the most descriptive and discriminative possible?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sparsity in itself is also a problem when computing cosine similarity (dot product) between documents in their vector space representation. Indeed, since the number of features in a single document is far off the total amount of features, the non-zero components are very rare. In the end, the probability to find common non-zero components in two document vectors is low.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sole feature extraction completely flattens the documents in terms of series of words or n-grams. These feature have no inner relationships. In natural languages, the reality is the opposite. Indeed, words do have relationships with each others, in terms of syntax but also in terms of topics. In addition to that polysemy and homonymy is very important. Polysemy is the fact that single words can be related to different meanings and topics. When not taken into account it leads to overestimating the similarity between documents. Similarly homonymy is the fact that several words can refer to the same meaning, and therefore lead to an underestimation of the true similarity between documents.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For overcoming these limits, researchers have come with solution that must be operated once features have been extracted and before performing comparisons and classification tasks on the documents. This is the object of topic modeling.&lt;/p&gt;
&lt;h3&gt;Latent semantic indexing (LSI)&lt;/h3&gt;
&lt;p&gt;The first dimensionality reduction technique, and the easiest to understand and implement as well, is the &lt;strong&gt;latent semantic indexing (LSI)&lt;/strong&gt;. It consists in using a singular value decomposition of the documents-terms matrix. Thus a linear subspace in the TF-IDF high dimensional space is found, which better captures the internal relationships between features. The singular value decomposition matrix, truncated to reduce to output space, known as the laten tspace, to around 300 dimensions. This allows to define new features which are linear combinations between the features of the input TF-IDF space.&lt;/p&gt;
&lt;p&gt;The components of the features in the can be positive and negative. Within a given feature, the highest positive components represent the TF-IDF features which at best describe the topic, while the highest negative components represent the TF-IDF features that are the most repulsive to that topic.&lt;/p&gt;
&lt;p&gt;LSI is very fast, and requires a low computational time. The algorithm is based on the Singular Value Decomposition of the TF-IDF matrix followed by a normalization. The SVD matrix is truncated to the size of the required feature space. Two matrices are then available: the documents-topics matrix and the topics-features matrix.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The first one is the linear mapping from documents to topics, with M documents and K topics. This matrix is dense and maps each documents linearly to the set of topics. This matrix with positive components is becomining our input data for the classification task, as it thought to be more consistant to the actual content of the text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second matrix is the linear mapping from the topics to the features, with N features and K topics. This matrix is used for retrieving the description of a topic, in terms of highest components.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Probabilistic Latent Semantic Indexing (pLSI)&lt;/h3&gt;
&lt;p&gt;Simple LSI have come with shortcomings. One immediate concern is the fact that features are linearly combined with positive and negative components. Negative components are difficult to interpret in terms of meanings for a topic.&lt;/p&gt;
&lt;p&gt;A second issue is that the LSI technique is non-generative for documents-words co-occurences; changing the nature of one of the documents will change the full LSI matrix. From the perspective of building a web service, this is a really annoying issue, in that the analysis cannot be reused by previously unseen documents, and even if the document are slightly modified.&lt;/p&gt;
&lt;p&gt;As it has been stated by Hofmann in &lt;a href="http://www.cs.helsinki.fi/u/vmakinen/stringology-k04/hofmann-unsupervised_learning_by_probabilistic_latent_semantic_analysis.pdf"&gt;is publication on pLSI&lt;/a&gt; LSI is unable to capture polysemy of the features. "The coordinates of the of a word in the latent space can be written as a linear superposition of the coordinates of the documents that contain the word". The fact that a word could have different meaning in different documents cannot be explained by this superposition model.&lt;/p&gt;
&lt;p&gt;Hofmann have thus built a technique close to the latent semantic analysis model but tackling these issues more seriously. The pLSI model adds a latent variable accounting for polysemy and related to each word occurrence. It is used to give information about the context of an occurrence, that is to say the other words also appearing in the document.&lt;/p&gt;
&lt;p&gt;&lt;img src="./images/plsi.png" title="'Plate representation of the PLSI model'" height="150" width="300" alt="'Plate representation of the PLSI model'" class="left half"&gt;&lt;/p&gt;
&lt;h3&gt;Latent Dirichlet Allocation&lt;/h3&gt;</summary></entry></feed>