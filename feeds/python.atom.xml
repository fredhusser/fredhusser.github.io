<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Data Science for collective intelligence</title><link href="https://fredhusser.github.io/" rel="alternate"></link><link href="https://fredhusser.github.io/feeds/python.atom.xml" rel="self"></link><id>https://fredhusser.github.io/</id><updated>2015-12-16T12:00:00+01:00</updated><entry><title>Part 3C - Topic modeling by semantic analysis in Python with Scikit-Learn</title><link href="https://fredhusser.github.io/semantic-indexing.html" rel="alternate"></link><updated>2015-12-16T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-16:semantic-indexing.html</id><summary type="html">&lt;h2&gt;Fundamentals on topic modeling&lt;/h2&gt;
&lt;p&gt;While the tf-idf technique, which we went through in &lt;a href="https://fredhusser.github.io/feature_extraction.html"&gt;the past post&lt;/a&gt;, is very efficient for extracting features that are discriminative to the documents, it suffers from several drawbacks and limitations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Straight feature extraction results in high dimensional and sparse feature spaces. With very conservative filtering parameters we came out with almost 5000 features describing our corpus of 2000 documents. Analyzing matrices of such size always comes always at high computational costs. Furthermore, in the field of machine learning, algorithms usually involve iterative training algorithms, that enhance the computational cost and might also be prone to overfitting. Dimensionality reduction is first and foremost a matter of efficiency. How could the input space be as small as possible while being the most descriptive and discriminative possible?&lt;/li&gt;
&lt;li&gt;Sparsity in itself is also a problem when computing cosine similarity (dot product) between documents in their vector space representation. Indeed, since the number of features in a single document is far off the total amount of features, the non-zero components are very rare. In the end, the probability to find common non-zero components in two document vectors is low. &lt;/li&gt;
&lt;li&gt;Sole feature extraction completely flattens the documents in terms of series of words or n-grams. These feature have no inner relationships. In natural languages, the reality is the opposite. Indeed, words do have relationships with each others, in terms of syntax but also in terms of topics. In addition to that polysemy and homonymy is very important. Polysemy is the fact that single words can be related to different meanings and topics. When not taken into account it leads to overestimating the similarity between documents. Similarly homonymy is the fact that several words can refer to the same meaning, and therefore lead to an underestimation of the true similarity between documents.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For overcoming these limits, researchers have come with solution that must be operated once features have been extracted and before performing comparisons and classification tasks on the documents. This is the object of topic modeling.&lt;/p&gt;
&lt;h3&gt;Latent semantic indexing (LSI)&lt;/h3&gt;
&lt;p&gt;The first dimensionality reduction technique, and the easiest to understand and implement as well, is the &lt;strong&gt;latent semantic indexing (LSI)&lt;/strong&gt;. It consists in using a singular value decomposition of the documents-terms matrix. Thus a linear subspace in the TF-IDF high dimensional space is found, which better captures the internal relationships between features. The singular value decomposition matrix, truncated to reduce to output space, known as the laten tspace, to around 300 dimensions. This allows to define new features which are linear combinations between the features of the input TF-IDF space. &lt;/p&gt;
&lt;p&gt;The components of the features in the can be positive and negative. Within a given feature, the highest positive components represent the TF-IDF features which at best describe the topic, while the highest negative components represent the TF-IDF features that are the most repulsive to that topic.&lt;/p&gt;
&lt;p&gt;LSI is very fast, and requires a low computational time. The algorithm is based on the Singular Value Decomposition of the TF-IDF matrix followed by a normalization. The SVD matrix is truncated to the size of the required feature space. Two matrices are then available: the documents-topics matrix and the topics-features matrix. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first one is the linear mapping from documents to topics, with M documents and K topics. This matrix is dense and maps each documents linearly to the set of topics. This matrix with positive components is becomining our input data for the classification task, as it thought to be more consistant to the actual content of the text.&lt;/li&gt;
&lt;li&gt;The second matrix is the linear mapping from the topics to the features, with N features and K topics. This matrix is used for retrieving the description of a topic, in terms of highest components.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Probabilistic Latent Semantic Indexing (pLSI)&lt;/h3&gt;
&lt;p&gt;Simple LSI have come with shortcomings. One immediate concern is the fact that features are linearly combined with positive and negative components. Negative components are difficult to interpret in terms of meanings for a topic.&lt;/p&gt;
&lt;p&gt;A second issue is that the LSI technique is non-generative for documents-words co-occurences; changing the nature of one of the documents will change the full LSI matrix. From the perspective of building a web service, this is a really annoying issue, in that the analysis cannot be reused by previously unseen documents, and even if the document are slightly modified.&lt;/p&gt;
&lt;p&gt;As it has been stated by Hofmann in &lt;a href="http://www.cs.helsinki.fi/u/vmakinen/stringology-k04/hofmann-unsupervised_learning_by_probabilistic_latent_semantic_analysis.pdf"&gt;is publication on pLSI&lt;/a&gt; LSI is unable to capture polysemy of the features. "The coordinates of the of a word in the latent space can be written as a linear superposition of the coordinates of the documents that contain the word". The fact that a word could have different meaning in different documents cannot be explained by this superposition model.&lt;/p&gt;
&lt;p&gt;Hofmann et al. have thus built a technique close to the latent semantic analysis model tackling these issues more seriously. The pLSI model adds a latent variable related to each single occurrence for a feature. It is used to give information about the context of the occurrence, that is to say the other words also appearing in the document. This helps accounting for polysemy. For instance, consider the word python. A first common sense is the snake, and a second possible meaning is the programming language. Given the other words co-occurring in the same document, it can be inferred whether the sense of the occurrence is the snake - for example when the word 'snake' is also in the document, or the right meaning is the programming language. In the last case, words like 'program', 'computer' or 'java' are likely to be present.&lt;/p&gt;
&lt;p&gt;A common representation of this algorithm is given in the following figure, found in the &lt;a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis"&gt;Wikipedia article about pLSI&lt;/a&gt;. This is a plate notation where the relationships between variables are displayed.
&lt;img src="./images/plsi.png" title="'Plate representation of the PLSI model'" height="150" width="300" alt="'Plate representation of the PLSI model'" class="left half"&gt;&lt;/p&gt;
&lt;p&gt;d is the document index variable. For each of the M documents, a word's topic c is extracted with a distribution over the N words w present in the full corpus. Using this notation, the co-occurrences are calculated in terms of probabilities P(w,d). These probabilities are decomposed in terms of conditional probabilities involving the latent variable c. The variable c is said to be latent in that it is not observable. In the application of topic modeling, the latent classes can be seen as topics as they create distributions of words likely to be present together in documents. &lt;/p&gt;
&lt;h3&gt;Latent Dirichlet Allocation&lt;/h3&gt;
&lt;p&gt;The implementation of topic modeling with the probabilistic Latent Semantic Indexing, was proved to have a serious limitation. It was &lt;a href="http://jmlr.csail.mit.edu/papers/v3/blei03a.html"&gt;shown by Blei, Ng and Jordan&lt;/a&gt; that this algorithm is not generative for documents. This means that the topic models which can be extracted for an analysis work poorly with un-seen documents. This is a serious issue in our perspective of classifying great amounts of documents: we need the analysis to be performed on a retricted set of documents while being able to apply the model to unseen documents whenever some are added to the corpus. &lt;/p&gt;
&lt;p&gt;The plate notation of LDA is shown below from the &lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;Wikipedia article about LDA&lt;/a&gt;.
&lt;img src="./images/lda.png" title="'Plate representation of the LDA model'" height="150" width="300" alt="'Plate representation of the LDA model'" class="left half"&gt;&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="Vector Space Model"></category><category term="Latent Semantic Indexing"></category><category term="Latent Dirichlet Allocation"></category></entry><entry><title>Part 3B - Text documents feature extraction and latent semantic analysis (LSA) with Scikit-Learn</title><link href="https://fredhusser.github.io/feature_extraction.html" rel="alternate"></link><updated>2015-12-15T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-15:feature_extraction.html</id><summary type="html">&lt;p&gt;In the &lt;a href="https://fredhusser.github.io/build_classifier.html"&gt;introduction post&lt;/a&gt; we have described the workflow we want to create for performing an automatic classification of text documents. These text documents have been crawled on lemonde.fr website.&lt;/p&gt;
&lt;h2&gt;Text vectorization&lt;/h2&gt;
&lt;p&gt;For vectorizing the corpus of articles we create a &lt;code&gt;Model&lt;/code&gt; child class called &lt;code&gt;ModelTFIDF&lt;/code&gt;. It uses the TF-IDF vectorization algorithm from Scikit-Learn. If you are not familiar to the Scikit-Learn interfaces,&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;the documentation&lt;/a&gt; can be consulted relatively to the &lt;code&gt;TfidfVectorizer&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;The underlying algorithm is based on a count vectorizer and a tf-idf transformer. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first tokenizes the text documents and makes a reference of all tokens with their appearance frequency in each document. A count matrix is built with tokens as columns and documents as rows. Since all words are not appearing in all documents the count matrix is very sparse. Especially when working with other languages than English, a very high level implementation of the tokenizer is available in the NLTK (Natural Language Toolkit). In a future step, a French tokenizer will be used. It applies a stemming algorithm, so that words like 'manger' and 'mangé' are not counted as different. Stemming can be highly beneficial in terms of computing time and also improves the relevance of the results.&lt;/li&gt;
&lt;li&gt;The tf-idf transformer operates a modulation of the in-document term frequencies by the log of the inverse of the document frequency for each feature. In other words, each time that a words appears at least once in a document of the corpus this is &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We first create the class, and override the metadata, namely the name of the output data. This is mainly required for producing consistant logs during the fit process.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ModelTFIDF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;transformer&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tfidf&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;output_data_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tfidfMatrix&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;mapper_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;words_to_index&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Obviously we need to override the &lt;code&gt;fit&lt;/code&gt; method. In this method we instantiate the vectorizer from Scikit-Learn with all the required parameters. Note that the object attribute &lt;code&gt;fit_parameters&lt;/code&gt; is set in the standardized &lt;code&gt;__init__&lt;/code&gt; method. Using the vectorizer from Scikit-Learn, we have to load the input data as a numpy array, which is the type requirement of the object attribute &lt;code&gt;input_data&lt;/code&gt; as instantiated in the &lt;code&gt;__init__&lt;/code&gt; method as well.&lt;/p&gt;
&lt;p&gt;The vectorizer outputs a sparse documents-features matrix. This will be assigned to our &lt;code&gt;output_data&lt;/code&gt; attributes. The &lt;code&gt;mapper_data&lt;/code&gt; attributes takes the vectorizer dictionary which maps the indexes of the features in the documents-features matrix and the string representation of the feature. In the most basic set-up a feature is a word. But it could also be bi-grams (two consecutive words), or more.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;language&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_set_tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;language&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;vectorizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_space_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="c"&gt;# Give access to the index of a word&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapper_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocabulary_&lt;/span&gt;

        &lt;span class="c"&gt;# Return a reverse mapper for retrieving a word from index&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapper_reverse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_space_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;a30&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapper_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapper_reverse&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_log_model_results&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;TfIdfVectorizer&lt;/code&gt; fit parameters are stored as Python dictionary in the &lt;code&gt;MLconfig.py&lt;/code&gt; module. The key-value pairs are then passed to the vectorizer instance in the fit method. The most important parameters at first are the &lt;code&gt;max_df&lt;/code&gt; and &lt;code&gt;min_df&lt;/code&gt;: they set the upper and lower thresholds for filtering out features considering their document frequencies. The document frequency is calculated relatively to the occurance in the corpus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Words or features appearing in all documents tend to be non discriminating in terms of documents comparisons. Usually these are what are called the stop-words: these are common to the language and grammar, and do not add much information on the content. In our example, we filter out features with more than 60% document frequency.&lt;/li&gt;
&lt;li&gt;Words or features appearing in a very small amount of documents tend to so specific that they cannot be used for a proper comparison. In the similarity measure that we will use in the classifier based on a dot product, they will constantly account for a multiplication by zero. Therefore they will be responsible a high computational while bringing low insight. In our example, we filter out features with less than 1% document frequency.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;PARAMETERS_VECTORIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;vocabulary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;max_df&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;min_df&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;ngram_range&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;  &lt;span class="c"&gt;# unigrams or bigrams&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;encoding&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;strip_accents&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;ascii&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;l2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using the workflow class defined in &lt;a href="https://fredhusser.github.io/build_classifier.html"&gt;Part A&lt;/a&gt;, we add a method for vectorizing the documents inputs. This method return the &lt;code&gt;self&lt;/code&gt; keyword for easing the chaining of processors in single command lines.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;vectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Representation of the text data into the vector space model.&lt;/span&gt;
&lt;span class="sd"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorize_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ModelTFIDF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corpus_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PARAMETERS_VECTORIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attributes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;corpus_size&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorize_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_set_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attributes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;n_features&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorize_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_space_size&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then a workflow object must be instantiated with the MongoDB connection parameters, and the &lt;code&gt;vectorizer&lt;/code&gt; method is operated. In a Python shell at the root of the subproject, you can fire-up the vectorization in one single command line. The log displays information about the processed job.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; from textmining.workflows import SemanticAnalysisWorkflow
&amp;gt;&amp;gt;&amp;gt; SemanticAnalysisWorkflow&lt;span class="o"&gt;(&lt;/span&gt;mongoclient, &lt;span class="s2"&gt;&amp;quot;scrapy&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;lemonde&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;.vectorizer&lt;span class="o"&gt;()&lt;/span&gt;
tfidf:  Model tfidf of &lt;span class="nb"&gt;type &lt;/span&gt;transformer was trained
tfidf:  Output name:     tfidfMatrix
tfidf:  Input space:     &lt;span class="o"&gt;(&lt;/span&gt;1958,&lt;span class="o"&gt;)&lt;/span&gt;
tfidf:  Output space:    &lt;span class="o"&gt;(&lt;/span&gt;1958, 4777&lt;span class="o"&gt;)&lt;/span&gt;
tfidf:  Model attributes:
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In our example, we have a corpus of 1958 text documents. After vectorization, a document matrix of shape 1958 documents by 4777 features. Obviously, the number of features is rather limited for such a high number of documents. However, for testing purposes, with the only goal to set up a full workflow this is more than enough. At the end of the iteration, we will be able to review the full process and make right decisions on rather or not we should filter so many features.&lt;/p&gt;
&lt;p&gt;Since the vectorizer returns the object itself, we could store it in a variable in order to reuse it later, with a second model. In the &lt;a href="https://github.com/fredhusser/collective-intelligence"&gt;Github repo&lt;/a&gt;, a IPython notebook can be used for better understanding the mechanics of the vectorization model.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="GENSIM"></category><category term="NLTK"></category><category term="TF-IDF"></category><category term="Vector Space Model"></category></entry><entry><title>Part 3A - Building an unsupervised classifier in Python with Scikit-Learn</title><link href="https://fredhusser.github.io/build_classifier.html" rel="alternate"></link><updated>2015-12-12T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-12:build_classifier.html</id><summary type="html">&lt;p&gt;In this tutorial, we tackle the first challenge of building a news article automatic classifier based on the documents content. On our last post, &lt;a href="https://fredhusser.github.io/web-crawling.html"&gt;we scraped some articles&lt;/a&gt; using Scrapy from &lt;a href="http://www.lemonde.fr"&gt;lemonde.fr&lt;/a&gt;, that were stored into a MongoDB instance. We want to classify all these articles, finding out groups of similar articles based on their topics. The constraint is that we do not know in advance the main topics coming up throughout te corpus. This should be infered from our analysis. &lt;/p&gt;
&lt;p&gt;Our second constraint, is that we want to be able to classify a large number of articles. Therefore the algorithms used should be scalable, but also in their design tolerate a large variety of topics. In order to ease browsing into the corpus by humans, we want to visualize graphically the result of the classification.&lt;/p&gt;
&lt;p&gt;We will define a basic workflow for building such a classifier in Python, using extensively Numpy, Pandas and Scikit-Learn. These packages are part of the Anaconda Python distribution from Continuum Analytics, which &lt;a href="https://fredhusser.github.io/development-environment.html"&gt;we installed in our virtual machine using Vagrant&lt;/a&gt;. From a more abstract perspective we will also propose a general template of a workflow chaining algorithms together. Each building block of the whole workflow is an algorithm operated on the dataset and producing an output which can be then in turn operated on by the following block.&lt;/p&gt;
&lt;p&gt;All the code featured in this page can be downloaded from &lt;a href="https://github.com/fredhusser/collective-intelligence"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Building blocks of the classifier&lt;/h2&gt;
&lt;p&gt;Our semantic, graphical and automatic classification algorithm will be built in four steps. This graph here shows the different steps the workflow must go through. From the architecture standpoint, we intend to build a worker performing machine learning tasks and which can be used as a web service. Therefore, its design has to enable it to be utilized as such. The inputs of the classifier should be in the end collected through a RESTful API, and so should be the outputs. However we will not focus on these matters at first. Priority is to build a working classifier, and we simply keep in mind these requirements.
&lt;img alt="Photo" src="https://fredhusser.github.io/images/workflow_v1.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Vectorization&lt;/strong&gt;: The first task is to extract features from the textual content. Vectorization and weighting is performed using the term-frequencies inverse document-frequencies method (TF-IDF): documents are vectorized, each token (a word or group of consecutive words) being a component, and the weights are proportional to the appearance frequency within the document and modulated by their inverse document frequency. This last steps ensures that terms appearing too often and therefore not discriminating enough, like stop words, can be filtered out. Similarly words appearing in too few documents and therefore too specific or rare can also be filtered out.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic analysis&lt;/strong&gt;: From the TF-IDF vectorization it is really common to have a documents-term matrix with several thousands different features. This results in a really noisy dataset: several words might be appearing with different stems and homonymy and polysemy is also a recurrent issue. In addition to this, high dimensionality comes with high computing cost at the analysis step. Dimensionality reduction using semantic analysis is our method to overcome these issues. At our disposal several techniques are available using matrix factorization techniques: latent semantic indexing (LSI), non-negative matrix factorization (NMF) and latent Dirichlet allocation (LDA). We will first try out the LSI since this algorithms is faster than the others, although it has some drawbacks. From the agile data analysis perspective, we want at first to build the full analysis pipeline, getting some insights on performance, and, only then, improve the models. In any of the algorithms stated above, groups of features are formed to build a set of representative topics. Each document is then assigned a distance to that topic in a documents-topic matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: The third task is to build the classifier based on the topics extracted before. This classifier must be unsupervised: no inputs from a samples or user interaction are required for training it. Instead it shall find by itself the main patterns in the training data set, and infer from the internal structure of the corpus a classification scheme. We use the Self-Organizing Maps (SOM) algorithm invented by T. Kohonen. This unsupervised kind of neural networks algorithms builds a mapping from the input features space into a 2D map space (set of adjacent nodes). This mapping preserves the topology of input space, and is therefore very appealing for its potential regarding data visualization. Besides, the nodes are represented by vectors of same dimensionality than the input space, and are trained to be similar to the training dataset vectors they map to.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: This property of SOM is used in a fourth step for building clusters of nodes. We use an agglomerative clustering algorithm for finding a limited number of clusters of adjacent and similar nodes. The Ward algorithm is matching the two conditions of adjacency and similarity for grouping two nodes together. However, this step is not applied on the dataset, but on the output of the classification algorithm. Complementary to it, its goal is to improve the readability of results by humans.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Structure of the master algorithm&lt;/h2&gt;
&lt;p&gt;In the introduction of this article we have mentioned the structure of the whole workflow. We basically define two abstract levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;Model&lt;/code&gt; class assemble an algorithm that operates on some input data and producing an output. Information about the dimensionality of the input and output space are also provided. The model is a standardized interface between algorithms so that they can be easily chained. It also contains a &lt;code&gt;fit()&lt;/code&gt; method that starts the training of the algorithms. Creating a model consists in creating a child class of the &lt;code&gt;Model&lt;/code&gt; class which overrides the &lt;code&gt;fit()&lt;/code&gt;method as well as the descriptive attributes. The class also has a method for streaming log messages, and other methods which are not displayed here:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sample_model&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;output_data_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;output_data&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;mapper_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;mapper_name&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;model_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fit_parameters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_space_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_parameters&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_set_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_space_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_space_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model_attributes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mapper_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;Worklfow&lt;/code&gt; class defines a master workflow that retrieves data from a source (SQL database or MongoDB store), applies to it a series of models and store the output of the classification in Mongo. In addition to that basic functionality this class must feature some helper methods for aggregating the data into Pandas &lt;code&gt;DataFrame&lt;/code&gt; instances for cleaning, reformatting, indexing, grouping and querying it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;SemanticAnalysisWorkflow&lt;/code&gt; is a class with a MongoDB extractor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SemanticAnalysisWorkflow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;MONGO_DBNAME_OUT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;textmining&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;MONGO_COLLECTION_OUT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;semantic&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;attributes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorize_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cluster_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_mongo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_mongo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;collection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;articles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;projection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;title&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;body&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

        &lt;span class="n"&gt;json_articles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;article&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;articles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;json_articles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;article&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;json_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_articles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;json_util&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corpus_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop_duplicates&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corpus_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the following sections we will cover the details about the four tasks we have stated in the introduction, that is vectorization, semantic analysis, classification and clustering.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="NLTK"></category><category term="TF-IDF"></category><category term="Vector Space Model"></category><category term="Latent Semantic Indexing"></category><category term="Latent Dirichlet Allocation"></category><category term="Self-organizing maps"></category><category term="Classification"></category><category term="Clustering"></category></entry><entry><title>Part 2 - Crawling a news website in Python with Scrapy and MongoDB</title><link href="https://fredhusser.github.io/web-crawling.html" rel="alternate"></link><updated>2015-12-11T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-11:web-crawling.html</id><summary type="html">&lt;h2&gt;Quick start with Scrapy&lt;/h2&gt;
&lt;p&gt;We will work on our virtual machine as defined in the previous part. The code that we will go through is available in the &lt;a href="https://github.com/fredhusser/collective_intelligence"&gt;GitHub repository&lt;/a&gt; under the worker_scrapy subdirectory. If the VM is not running just type &lt;code&gt;vagrant up&lt;/code&gt; and then &lt;code&gt;vagrant ssh&lt;/code&gt; while being at the root of the project. Consult this &lt;a href="https://fredhusser.github.io/development-environment.html"&gt;post&lt;/a&gt; for learning how to create this VM using Vagrant.&lt;/p&gt;
&lt;p&gt;Since Scrapy will require some additional packages it is convenient to use a virtual python environment dedicated to our Scrapy worker. For doing this we will simply use the Conda package manager, and type in the terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; /vagrant/worker_scrapy
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo conda create -n scrapyenv -y scrapy pymongo
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will create a python virtual environment named &lt;strong&gt;scrapyenv&lt;/strong&gt;, in which we will work with Scrapy and pymongo. For activating the environment just type in the bash:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;source &lt;/span&gt;activate scrapyenv
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In your terminal, each line should now start with &lt;strong&gt;(scrapyenv)&lt;/strong&gt;. We are now ready to build a crawler with scrapy.&lt;/p&gt;
&lt;h2&gt;Project Structure&lt;/h2&gt;
&lt;p&gt;At the root of our scraper project we create a python package called scraper in which we will pack all the code. The file scrapy.cfg states the location of the scraper settings module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[settings]&lt;/span&gt;
&lt;span class="na"&gt;default&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;scraper.settings&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The scraper package contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A package containing the code of a &lt;strong&gt;spider&lt;/strong&gt;. The spider is crawling into a website, selects web pages from all the internal hyperlinks according to selection rules, and collects items in the html pages according to their css classes or html tags.&lt;/li&gt;
&lt;li&gt;A module called &lt;strong&gt;items.py&lt;/strong&gt; defines the model of the object we want to extract. Defined as Python classes, the &lt;code&gt;Item&lt;/code&gt; objects are shaped according to the data we want to extract.&lt;/li&gt;
&lt;li&gt;A module called &lt;strong&gt;pipelines.py&lt;/strong&gt; defines the pipeline of actions to be performed once an item was extracted from a web page. In our case it will consist in storing the data into our MongoDB instance.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;settings.py&lt;/strong&gt; module defines all the parameters that will be needed in the scraping process. It tells Scrapy which spiders to use, states which pipelines must be followed and also defines the database connection settings.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The file Structure of the projects looks therefore as following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;scraper
  spiders
    __init__.py
    webarticles.py
  __init__.py
  items.py
  pipelines.py
  settings.py
scrapy.cfg
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Defining the model of the items&lt;/h3&gt;
&lt;p&gt;The Item class is defined in the Scrapy internals. We will create a child class that will contains the fields we want to extract from each lemonde.fr webpage: the title, the article body and the publication timestamp.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scrapy.item&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Item&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LeMondeArt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Item&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Define the model of the articles extracted from the website.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;body&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This class models the items we want to extract, and this extraction is operated by the spider. &lt;/p&gt;
&lt;h3&gt;Defining the spider for lemonde.fr&lt;/h3&gt;
&lt;p&gt;There are different spiders base classes defined in Scrapy. We will use the CrawlSpider class as it contains procedures to extract data in the html page and to follow the hyperlinks. Thanks to class inheritance we only need to state the extraction rules, the parsing of the html data. We also create a method parse_article that tells Scrapy backend where to and how to get data in the html tree.&lt;/p&gt;
&lt;p&gt;The class is looking like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LeMondeSpider&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CrawlSpider&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lemonde&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;allowed_domains&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lemonde.fr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;start_urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.lemonde.fr/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;article_item_fields&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.//article/h1/text()&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;#39;timestamp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.//article/p[@class=&amp;quot;bloc_signature&amp;quot;]/time[@itemprop=&amp;quot;datePublished&amp;quot;]/@datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;#39;body&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.//article/div[@id=&amp;quot;articleBody&amp;quot;]/*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;rules&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Rule&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LinkExtractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;allow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;r&amp;quot;article/\d{4}/\d{2}/\d{2}/.+&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;parse_article&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;follow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The dictionary &lt;code&gt;article_item_fields&lt;/code&gt; links each field of the &lt;code&gt;LeMondeItem&lt;/code&gt; to a &lt;strong&gt;xpath&lt;/strong&gt; selector telling where to get the data in the HTML tree. The &lt;code&gt;rules&lt;/code&gt; tuple tells how the URLs allowed to be followed look like. It helps for instance restricting the crawler on all the pages referring to articles.&lt;/p&gt;
&lt;p&gt;Then we add to this class a method for parsing data from the web page: it features a Selector, a Loader and a Processor object. When the data is extracted this method also define how a new LeMondeItem is created and populated with fields. Explaining in details the internals of Scrapy is out of the scope of this tutorial. However, you can refer to the official documentation.&lt;/p&gt;
&lt;p&gt;Add this method to the LeMondeSpider method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_article&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Selector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XPathItemLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LeMondeArt&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s"&gt;A response from &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt; just arrived!&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c"&gt;# define processors&lt;/span&gt;
        &lt;span class="n"&gt;text_input_processor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MapCompose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;unicode&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;default_output_processor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Join&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c"&gt;# Populate the LeMonde Item with the item loader&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xpath&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;article_item_fields&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;loader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_xpath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;field&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_input_processor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;XPath &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt; not found at url &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;loader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that a spider is defined for a given website, as it fits to the HTML tags and CSS classes. However, thanks to the model defined in the &lt;code&gt;items.py&lt;/code&gt; module our data is generalized so that the same king of fields could be extracted from any other news website. To do that, it is just needed to create another CrawlSpider class, and adapt the fields.&lt;/p&gt;
&lt;h3&gt;Defining the data storage pipelines&lt;/h3&gt;
&lt;p&gt;In the module &lt;code&gt;pipelines.py&lt;/code&gt; we tell scrapy what to do when an item instance was created from a spider instance. Our pipeline only consists in processing the item fields and store them in a MongoDB. The class &lt;code&gt;__init__&lt;/code&gt; method creates a connection to the MongoDB service from the connection settings using the &lt;code&gt;pymongo&lt;/code&gt; driver.&lt;/p&gt;
&lt;p&gt;The pipeline is generally defined and operates principally on the item instances. We use the &lt;strong&gt;bleach&lt;/strong&gt; package for cleaning the HTML text in the body. If you plan to make another pipeline object, make sure that you declare the &lt;code&gt;process_item&lt;/code&gt; as it is required by Scrapy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;tags&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;h2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;p&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;em&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;strong&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WebArticlesPipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;A pipeline for storing scraped items in the database&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;MONGODB_COLLECTION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lemonde&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;collection_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lemonde&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pymongo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MongoClient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;settings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;MONGODB_SERVER_HOST&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="n"&gt;settings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;MONGODB_SERVER_PORT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;settings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;MONGODB_DB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;collection_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_item&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;spider&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;+&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;body&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bleach&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;body&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;tags&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
                &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;DropItem&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Missing {0}!&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Question added to MongoDB database&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;level&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;spider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;spider&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Defining the settings&lt;/h3&gt;
&lt;p&gt;The settings module simply contains all the set of constants required for the crawling process: declare the spider settings, the pipelines to use and the database connection details.&lt;/p&gt;
&lt;h3&gt;Fire-up crawling&lt;/h3&gt;
&lt;p&gt;Once you are all set, from the root of the subdirectory, where the &lt;code&gt;scrapy.conf&lt;/code&gt; file is located you can start the crawler from the command line. The syntax is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;scrapy crawl lemonde
&lt;/pre&gt;&lt;/div&gt;</summary><category term="Scrapy"></category><category term="MongoDB"></category><category term="Virtual environment"></category><category term="Python"></category></entry><entry><title>Part 1 - Python Data Analysis environment with Vagrant, Docker and Anaconda</title><link href="https://fredhusser.github.io/development-environment.html" rel="alternate"></link><updated>2015-12-10T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-10:development-environment.html</id><summary type="html">&lt;p&gt;In this tutorial, we build an environment with Vagrant, Docker and Anaconda for doing firstly data analysis in Python. It should be totally agnostic on the choice of your operating system, be it Linux, Mac or Windows. Vagrant is used for configuring the linux Ubuntu virtual machine. The vagrant file located at the root of the project directory contains all the settings for that VM so that it can boot with all dependencies provisioned &lt;/p&gt;
&lt;p&gt;Before starting make sure that you do have this three softwares installed on your machine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VirtualBox&lt;/li&gt;
&lt;li&gt;Vagrant&lt;/li&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Configuring a Linux Ubuntu machine&lt;/h2&gt;
&lt;p&gt;Create a directory in which you will start the project. Of course the repository with all the code is available on GitHub, so that you can get started faster by cloning it.&lt;/p&gt;
&lt;p&gt;In order to stress on the most important parts of our setup, we will go through the configuration ste-by-step.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;mkdir collective-intelligence
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd &lt;/span&gt;collective-intelligence
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will create a Vagrant configuration file Vagrantfile that will contain all information necessary to build and run the virtual machine. We setup the name of the VM, the network settings and port forwarding. We also add a synchronized folder so that all changes in the file system of the host machine are taken into account in the file system of the guest. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;VAGRANTFILE_API_VERSION = &amp;quot;2&amp;quot;
Vagrant.configure(2) do |config|
    config.vm.box = &amp;quot;ubuntu/trusty64&amp;quot;
    config.vm.hostname = &amp;quot;vagrant-docker-anaconda&amp;quot;
    config.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.50.100&amp;quot;
    config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 80, host: 8080
    config.vm.provider :virtualbox do |vb|
        vb.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--memory&amp;quot;, &amp;quot;1024&amp;quot;, &amp;quot;--cpus&amp;quot;, &amp;quot;2&amp;quot;]
    end

    config.vm.synced_folder &amp;quot;.&amp;quot;, &amp;quot;/vagrant&amp;quot;, :type =&amp;gt; &amp;quot;nfs&amp;quot;
    end
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once the Vagrantfile is defined you are able to build and run the VM simply by typing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;vagrant up
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Provisioning with the necessary software&lt;/h2&gt;
&lt;p&gt;For starting developping applications for data analysis we need a Python distribution. For data analysis in Python, Anaconda offered by Continuum Analytics is very convenient, and contains all the necessary packages. First connect through ssh to your running VM in one command line.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;vagrant ssh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we will install anaconda. From the bash terminal connected to your VM, you can install Anaconda&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;sudo apt-get update

&lt;span class="c"&gt;# Download from the Anaconda remote directory&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo wget http://09c8d0b2229f813c1b93-c95ac804525aac4b6dba79b00b39d1d3.r79.cf1.rackcdn.com/Anaconda-2.1.0-Linux-x86_64.sh

&lt;span class="c"&gt;# Install the Anaconda distribution in batch mode&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo bash Anaconda-2.1.0-Linux-x86_64.sh -b

&lt;span class="c"&gt;# Append the path of the Anaconda Python interpreter into your path&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/home/vagrant/anaconda/bin:&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="c"&gt;# Update the package manager and install pymongo as we will use it&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo conda update -y conda
&lt;span class="nv"&gt;$ &lt;/span&gt;sudo conda install -y pymongo
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you are all set and ready to work with Python. You can check your installation by starting the Python interpreter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;vagrant@vagrant-docker-anaconda:~&lt;span class="nv"&gt;$ &lt;/span&gt;python
Python 2.7.10 &lt;span class="p"&gt;|&lt;/span&gt;Anaconda 2.1.0 &lt;span class="o"&gt;(&lt;/span&gt;64-bit&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;default, Oct &lt;span class="m"&gt;19&lt;/span&gt; 2015, 18:04:42&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;GCC 4.4.7 &lt;span class="m"&gt;20120313&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;Red Hat 4.4.7-1&lt;span class="o"&gt;)]&lt;/span&gt; on linux2
Type &lt;span class="s2"&gt;&amp;quot;help&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;copyright&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;credits&amp;quot;&lt;/span&gt; or &lt;span class="s2"&gt;&amp;quot;license&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
&amp;gt;&amp;gt;&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Using Docker for our database and web application&lt;/h2&gt;
&lt;p&gt;Now we are almost ready to develop our machine learning application ! We also want to have a MongoDB and Postgres SQL instances running, as well as a web application (in Python of course!). We will use Docker and Docker-Compose for that and build a multi-container application. Basically we want to have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;MongoDB&lt;/strong&gt; server that will be accessible on the localhost of our VM, in which we will store our collected data and the outputs of the machine learning tasks;&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;PostgresSQL&lt;/strong&gt; server that will support our web application data;&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Flask&lt;/strong&gt; back-end for our application, which we will use for communicating our results to the outside world;&lt;/li&gt;
&lt;li&gt;An &lt;strong&gt;nginx&lt;/strong&gt; service to forward requests either to the Flask app or the static files;&lt;/li&gt;
&lt;li&gt;Some &lt;strong&gt;data-only containers&lt;/strong&gt; that will back-up the data from the database services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out this blog post from &lt;a href="https://realpython.com/blog/python/dockerizing-flask-with-compose-and-machine-from-localhost-to-the-cloud/"&gt;Real Python&lt;/a&gt; which explains the process of building a muti-container application with flask, and from which we got inspired to build the docker-compose.yml file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;mongo&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;restart&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;always&lt;/span&gt;
  &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mongo&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="n"&gt;volumes_from&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;data_mongo&lt;/span&gt;
  &lt;span class="n"&gt;ports&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;27017:27017&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;expose&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;27017&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;data_mongo&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mongo&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/vagrant/data/mongo:/var/lib/mongo:rw&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;entrypoint&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/bin/true&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to make Vagrant able to run the containers as soon it starts up, you have to provision Docker and Docker-Compose from the Vagrantfile, in which you must add the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;config.vm.provision :docker
config.vm.provision :docker_compose, yml: &amp;quot;/vagrant/docker-compose.yml&amp;quot;, rebuild: true, run: &amp;quot;always&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;By reloading the VM this will be taken into account and the docker client will try to build our MongoDB container. If you look directly into the repository you will notice that the other containers (nginx, Flask, PostgresSQL) are also defined. However it is out of the scope of this first set-up step to explain in details how to build them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;vagrant reload
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the VM reloads successfully, it can take few minutes for Docker to set-up, you can check that the MongoDB container is effectively there. First reconnect by ssh to your VM and test your docker:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;vagrant@vagrant-docker-anaconda:~&lt;span class="nv"&gt;$ &lt;/span&gt;docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES
0c7cc8c4d3cd        mongo:3.0.2         &lt;span class="s2"&gt;&amp;quot;/entrypoint.sh mongo&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;10&lt;/span&gt; days ago         Up &lt;span class="m"&gt;5&lt;/span&gt; hours          0.0.0.0:27017-&amp;gt;27017/tcp   vagrant_mongo_1
&lt;/pre&gt;&lt;/div&gt;</summary><category term="Vagrant"></category><category term="Docker"></category><category term="Python"></category><category term="Anaconda"></category></entry><entry><title>Part 0 - Introduction to Programming Collective in Python</title><link href="https://fredhusser.github.io/introduction-collective-intelligence.html" rel="alternate"></link><updated>2015-12-09T12:00:00+01:00</updated><author><name>Frederic Husser</name></author><id>tag:fredhusser.github.io,2015-12-09:introduction-collective-intelligence.html</id><summary type="html">&lt;p&gt;This blog is also meant to be a recipe for data scientists and physicists who want to be able to present and communicate about their work, so that their algorithms and analysis do not sleep forever deep in their GitHub account.&lt;/p&gt;
&lt;h2&gt;So what are the goals we want to pursue?&lt;/h2&gt;
&lt;p&gt;We want to build an application that collects a great number of blog posts, news articles or opinions in the web, use a semantic analysis to classify them based on content and allows users to browse faster in the corpus through a simple web application.&lt;/p&gt;
&lt;p&gt;In this blog we intend to present some tools for doing data analysis with Python but also we want to propose a way to conduct data analysis projects. The goals are to reduce the frictions in the full process. Frictions stem usually from the difficulty to work together for people with different backgrounds and fields: developers, data scientists, users and consummers. &lt;/p&gt;
&lt;p&gt;The lean data science is based upon the principle that data collection, analysis and visualization should be considered as a loop that must be completed at the fastest possible speed. At each iteration, developers and data scientists can work collaboratively to improve the solution at any level as they can see the full picture. So let's get to work.&lt;/p&gt;
&lt;h2&gt;Mission, objectives and roadmap&lt;/h2&gt;
&lt;h3&gt;Mission&lt;/h3&gt;
&lt;p&gt;Our mission is to build a intelligent news web application able to classify and cluster articles and opinions. It must offer to users a simple but complete browsing experience, so that they can reach the biggest amount of relevant content in the least amount of time. Documents relevance is assessed using:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;semantic analysis&lt;/strong&gt;, that is to say extract features from the textual content, &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;collaborative filtering&lt;/strong&gt; that is to say extract features on documents based on how users interact with them (likes, shares, comments).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Objectives&lt;/h3&gt;
&lt;p&gt;In order to realize this mission we can set up few objectives and constraints. First and foremost, we want to be &lt;strong&gt;open source&lt;/strong&gt; and &lt;strong&gt;transparent&lt;/strong&gt;. When using machine learning algorithms with the aim of recommending content to users it is immensely necessary to be open to critics so that it acknowledged by users to be fair and not pursuing self-interests. &lt;/p&gt;
&lt;p&gt;Secondly, we do not seek the ultimate model and algorithm that will unequivocally resolve the challenge. Instead, algorithms must be combined, confronted and strongly challenged. There is no debate on whether model-based recommendations are better that collaborative filtering when it comes to recommending content. Instead, it must be recognized that both have strengths and weeknesses and therefore the question is how to make them operate together.&lt;/p&gt;
&lt;p&gt;Finally, we want to offer to users the capability to browse quickly in the documents although the corpus is meant to be important. The classification and clustering algorithms must integrate this objective so that documents indexing is perfectly matching the requirements of a smart data visualization.&lt;/p&gt;
&lt;h3&gt;Roadmap&lt;/h3&gt;
&lt;p&gt;In the first iteration we will tackle the issue of semantic analysis for classifying documents based on their content. In order to achieve this goal we will operate in an agile manner so that we complete the following steps the fastest possible.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Environment set-up&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will set up a reproducible development environment, with all the basic tools and frameworks we are going to need. For this we will be using Vagrant for taking care of our virtual machine on which to run Python scripts and Docker containers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Then some input data will be scraped from the web in order to have a collection of articles on which to perform some analysis. We will use Scrapy, a web scraper in made in Python, to help us crawling articles from &lt;a href="http://www.lemonde.fr"&gt;&lt;em&gt;www.lemonde.fr&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data analytics and text mining&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here is the hard work! Standing on the shoulder of giants like Numpy, NLTK and Scikit-Learn, we will build a document classification algorithm based on semantic analysis and self-organizing maps. We will rely a lot on the existing algorithms for natural language processing (NLP) such as in Scikit-Learn and NLTK, and we will eventually build our own self-organizing map classifier in Python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data visualization and browsing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Building a light weight web application with Flask and pymongo, we will make possible to data scientists to communicate on their results, with a step-by-step workflow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Present atomic records or chunks of the data; in our case it will be the news articles crawled from lemonde.fr. This step is really important to complete so that you can also evaluate the quality of your raw data. Keep the iteration based mindset.&lt;/li&gt;
&lt;li&gt;Build data visualizations for presenting the results of your analysis, enabling the user to understand the full scope of it. We want also to make the data scientists able to evaluate the quality of the analysis they performed with an objective point of view.&lt;/li&gt;
&lt;li&gt;Create reports that convey a message understandable by your audience. These reports are based on the analysis you conducted. For instance, what are the hot topics, the most transversal articles or the most specific ones? The value of your analysis is derived from the insight on the data it can give and the actions that can be taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="dev environment"></category><category term="Machine Learning"></category><category term="Agile Data Science"></category></entry></feed>